{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFtZwcNuYAe390L4iSJgg2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oreomcflurryyy/statistical-learning-deeplearning/blob/main/deeplearning_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOxsZys5I7YY",
        "outputId": "98f20e7c-fc02-4386-c44a-6c99c6c03cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ISLP\n",
            "  Downloading ISLP-0.4.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from ISLP) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.10/dist-packages (from ISLP) (1.13.1)\n",
            "Requirement already satisfied: pandas>=0.20 in /usr/local/lib/python3.10/dist-packages (from ISLP) (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from ISLP) (5.3.0)\n",
            "Requirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.10/dist-packages (from ISLP) (1.6.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from ISLP) (1.4.2)\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from ISLP) (0.14.4)\n",
            "Collecting lifelines (from ISLP)\n",
            "  Downloading lifelines-0.30.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pygam (from ISLP)\n",
            "  Downloading pygam-0.9.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from ISLP) (2.5.1+cu121)\n",
            "Collecting pytorch-lightning (from ISLP)\n",
            "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting torchmetrics (from ISLP)\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20->ISLP) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20->ISLP) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20->ISLP) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2->ISLP) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13->ISLP) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13->ISLP) (24.2)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from lifelines->ISLP) (3.10.0)\n",
            "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.10/dist-packages (from lifelines->ISLP) (1.7.0)\n",
            "Collecting autograd-gamma>=0.3 (from lifelines->ISLP)\n",
            "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting formulaic>=0.2.2 (from lifelines->ISLP)\n",
            "  Downloading formulaic-1.1.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: progressbar2<5.0.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pygam->ISLP) (4.5.0)\n",
            "Collecting scipy>=0.9 (from ISLP)\n",
            "  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m228.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning->ISLP) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning->ISLP) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning->ISLP) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning->ISLP)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->ISLP) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->ISLP) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->ISLP) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->ISLP) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->ISLP) (1.3.0)\n",
            "Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines->ISLP)\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines->ISLP) (1.17.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (3.11.11)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning->ISLP) (75.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines->ISLP) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines->ISLP) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines->ISLP) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines->ISLP) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines->ISLP) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines->ISLP) (3.2.1)\n",
            "Requirement already satisfied: python-utils>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from progressbar2<5.0.0,>=4.2.0->pygam->ISLP) (3.9.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.20->ISLP) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->ISLP) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (3.10)\n",
            "Downloading ISLP-0.4.0-py3-none-any.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lifelines-0.30.0-py3-none-any.whl (349 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.3/349.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygam-0.9.1-py3-none-any.whl (522 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m522.0/522.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading formulaic-1.1.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Downloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: autograd-gamma\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4031 sha256=37a77fca44650a1906a8acd278664e99a90ca941d3cdebaf8b25d01a88d81a49\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/cc/e0/ef2969164144c899fedb22b338f6703e2b9cf46eeebf254991\n",
            "Successfully built autograd-gamma\n",
            "Installing collected packages: scipy, lightning-utilities, interface-meta, autograd-gamma, torchmetrics, pygam, formulaic, lifelines, pytorch-lightning, ISLP\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "Successfully installed ISLP-0.4.0 autograd-gamma-0.5.0 formulaic-1.1.1 interface-meta-1.3.0 lifelines-0.30.0 lightning-utilities-0.11.9 pygam-0.9.1 pytorch-lightning-2.5.0.post0 scipy-1.11.4 torchmetrics-1.6.1\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.5.0.post0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.10.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.11.9)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.11)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.1+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ISLP\n",
        "!pip install pytorch-lightning\n",
        "!pip install torchmetrics\n",
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd\n",
        "from matplotlib.pyplot import subplots\n",
        "from sklearn.linear_model import \\\n",
        "     (LinearRegression,\n",
        "      LogisticRegression,\n",
        "      Lasso)\n",
        "from sklearn.metrics import (r2_score, mean_squared_error)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from ISLP import load_data\n",
        "from ISLP.models import ModelSpec as MS\n",
        "from sklearn.model_selection import \\\n",
        "     (train_test_split,\n",
        "      GridSearchCV)"
      ],
      "metadata": {
        "id": "ekYLcti2LHjy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import RMSprop\n",
        "from torch.utils.data import TensorDataset"
      ],
      "metadata": {
        "id": "6du0DgjRLL5H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import (MeanAbsoluteError,\n",
        "                          R2Score)\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "id": "TR3KelZCLN2x"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.loggers import CSVLogger"
      ],
      "metadata": {
        "id": "W-_jDtxTLP46"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning import seed_everything\n",
        "seed_everything(0, workers=True)\n",
        "torch.use_deterministic_algorithms(True, warn_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43UqxG7cLR2c",
        "outputId": "cfd49211-5f7f-43e8-cfe5-b30120bed99a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.io import read_image\n",
        "from torchvision.datasets import MNIST, CIFAR100\n",
        "from torchvision.models import (resnet50,\n",
        "                                ResNet50_Weights)\n",
        "from torchvision.transforms import (Resize,\n",
        "                                    Normalize,\n",
        "                                    CenterCrop,\n",
        "                                    ToTensor)"
      ],
      "metadata": {
        "id": "FyIkAjI8LTyc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ISLP.torch import (SimpleDataModule,\n",
        "                        SimpleModule,\n",
        "                        ErrorTracker,\n",
        "                        rec_num_workers)"
      ],
      "metadata": {
        "id": "_Nj9F_S7LXkX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ISLP.torch.imdb import (load_lookup,\n",
        "                             load_tensor,\n",
        "                             load_sparse,\n",
        "                             load_sequential)"
      ],
      "metadata": {
        "id": "rRfOVosqLYSm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat the analysis of Lab 10.9.5 on the IMDb data using a similarly\n",
        "structured neural network. We used 16 hidden units at each of two\n",
        "hidden layers. Explore the effect of increasing this to 32 and 64 units\n",
        "per layer, with and without 30% dropout regularization."
      ],
      "metadata": {
        "id": "G4LNqfjKwyho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(imdb_seq_train,\n",
        " imdb_seq_test) = load_sequential(root='data/IMDB')\n",
        "padded_sample = np.asarray(imdb_seq_train.tensors[0][0])\n",
        "sample_review = padded_sample[padded_sample > 0][:12]\n",
        "sample_review[:12]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4x4c9hRPDfV",
        "outputId": "c0502214-808d-4ea4-f90e-3ce11f45de74"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving \"IMDB_S_train.tensor.gz\" from \"http://imdb.jtaylor.su.domains/jtaylor/data/\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ISLP/torch/imdb.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  S_test) = [torch.load(_get_imdb(f'IMDB_{r}', root))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving \"IMDB_S_test.tensor.gz\" from \"http://imdb.jtaylor.su.domains/jtaylor/data/\".\n",
            "Retrieving \"IMDB_Y_test.npy\" from \"http://imdb.jtaylor.su.domains/jtaylor/data/\".\n",
            "Retrieving \"IMDB_Y_train.npy\" from \"http://imdb.jtaylor.su.domains/jtaylor/data/\".\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1,   14,   22,   16,   43,  530,  973, 1622, 1385,   65,  458,\n",
              "       4468], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lookup = load_lookup(root='data/IMDB')\n",
        "' '.join(lookup[i] for i in sample_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "o_lXGr19PoRP",
        "outputId": "53494153-802a-4a44-cef0-8ab7616ede41"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving \"IMDB_word_index.pkl\" from \"http://imdb.jtaylor.su.domains/jtaylor/data/\".\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<START> this film was just brilliant casting location scenery story direction everyone's\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_num_workers=10\n",
        "(imdb_train,\n",
        " imdb_test) = load_tensor(root='data/IMDB')\n",
        "imdb_dm = SimpleDataModule(imdb_train,\n",
        "                           imdb_test,\n",
        "                           validation=2000,\n",
        "                           num_workers=min(6, max_num_workers),\n",
        "                           batch_size=512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSQw9l7YPxvV",
        "outputId": "d3abd605-ee00-468f-8e07-0f6532aa6440"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving \"IMDB_X_test.tensor.gz\" from \"http://imdb.jtaylor.su.domains/jtaylor/data/\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ISLP/torch/imdb.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  X_test, X_train = [torch.load(_get_imdb(f'IMDB_{r}', root))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving \"IMDB_X_train.tensor.gz\" from \"http://imdb.jtaylor.su.domains/jtaylor/data/\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size):\n",
        "        super(IMDBModel, self).__init__()\n",
        "        self.dense1 = nn.Linear(input_size, 32) # Changed to 32 units\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3) # Dropout regularisation\n",
        "        self.dense2 = nn.Linear(32, 32) # Changed to 32 units\n",
        "        self.output = nn.Linear(32, 1) # Changed to 32 units\n",
        "\n",
        "    def forward(self, x):\n",
        "        val = x\n",
        "        for _map in [self.dense1,\n",
        "                     self.activation,\n",
        "                     self.dropout,\n",
        "                     self.dense2,\n",
        "                     self.activation,\n",
        "                     self.dropout,\n",
        "                     self.output]:\n",
        "            val = _map(val)\n",
        "        return torch.flatten(val)"
      ],
      "metadata": {
        "id": "rfsCqcNrP20R"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_model = IMDBModel(imdb_test.tensors[0].size()[1])\n",
        "summary(imdb_model,\n",
        "        input_size=imdb_test.tensors[0].size(),\n",
        "        col_names=['input_size',\n",
        "                   'output_size',\n",
        "                   'num_params'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtHFarmRQjgo",
        "outputId": "4f4d9483-8276-4560-ba7b-d56c8ad47d53"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===================================================================================================================\n",
              "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
              "===================================================================================================================\n",
              "IMDBModel                                [25000, 10003]            [25000]                   --\n",
              "├─Linear: 1-1                            [25000, 10003]            [25000, 32]               320,128\n",
              "├─ReLU: 1-2                              [25000, 32]               [25000, 32]               --\n",
              "├─Dropout: 1-3                           [25000, 32]               [25000, 32]               --\n",
              "├─Linear: 1-4                            [25000, 32]               [25000, 32]               1,056\n",
              "├─ReLU: 1-5                              [25000, 32]               [25000, 32]               --\n",
              "├─Dropout: 1-6                           [25000, 32]               [25000, 32]               --\n",
              "├─Linear: 1-7                            [25000, 32]               [25000, 1]                33\n",
              "===================================================================================================================\n",
              "Total params: 321,217\n",
              "Trainable params: 321,217\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 8.03\n",
              "===================================================================================================================\n",
              "Input size (MB): 1000.30\n",
              "Forward/backward pass size (MB): 13.00\n",
              "Params size (MB): 1.28\n",
              "Estimated Total Size (MB): 1014.58\n",
              "==================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_optimizer = RMSprop(imdb_model.parameters(), lr=0.001)\n",
        "imdb_module = SimpleModule.binary_classification(\n",
        "                         imdb_model,\n",
        "                         optimizer=imdb_optimizer)"
      ],
      "metadata": {
        "id": "B7NChBciRJ0x"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_logger = CSVLogger('logs', name='IMDB')\n",
        "imdb_trainer = Trainer(deterministic=True,\n",
        "                       max_epochs=30,\n",
        "                       logger=imdb_logger,\n",
        "                       enable_progress_bar=False,\n",
        "                       callbacks=[ErrorTracker()])\n",
        "imdb_trainer.fit(imdb_module,\n",
        "                 datamodule=imdb_dm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qedxFV9aRMm9",
        "outputId": "d4a5e763-41f1-4231-e065-7b0be13547cc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type              | Params | Mode \n",
            "----------------------------------------------------\n",
            "0 | model | IMDBModel         | 321 K  | train\n",
            "1 | loss  | BCEWithLogitsLoss | 0      | train\n",
            "----------------------------------------------------\n",
            "321 K     Trainable params\n",
            "0         Non-trainable params\n",
            "321 K     Total params\n",
            "1.285     Total estimated model params size (MB)\n",
            "7         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=30` reached.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = imdb_trainer.test(imdb_module, datamodule=imdb_dm)\n",
        "test_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "0MBG2fwWR5FP",
        "outputId": "505f3a4d-90c0-434f-9771-bb1938142fde"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8643199801445007    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9321557879447937    \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8643199801445007     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9321557879447937     </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'test_loss': 0.9321557879447937, 'test_accuracy': 0.8643199801445007}]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size):\n",
        "        super(IMDBModel, self).__init__()\n",
        "        self.dense1 = nn.Linear(input_size, 64) # Changed to 64 units\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dense2 = nn.Linear(64, 64) # Changed to 64 units\n",
        "        self.output = nn.Linear(64, 1) # Changed to 64 units\n",
        "\n",
        "    def forward(self, x):\n",
        "        val = x\n",
        "        for _map in [self.dense1,\n",
        "                     self.activation,\n",
        "                     self.dense2,\n",
        "                     self.activation,\n",
        "                     self.output]:\n",
        "            val = _map(val)\n",
        "        return torch.flatten(val)\n",
        "imdb_model = IMDBModel(imdb_test.tensors[0].size()[1])\n",
        "summary(imdb_model,\n",
        "        input_size=imdb_test.tensors[0].size(),\n",
        "        col_names=['input_size',\n",
        "                   'output_size',\n",
        "                   'num_params'])\n",
        "imdb_optimizer = RMSprop(imdb_model.parameters(), lr=0.001)\n",
        "imdb_module = SimpleModule.binary_classification(\n",
        "                         imdb_model,\n",
        "                         optimizer=imdb_optimizer)\n",
        "imdb_logger = CSVLogger('logs', name='IMDB')\n",
        "imdb_trainer = Trainer(deterministic=True,\n",
        "                       max_epochs=30,\n",
        "                       logger=imdb_logger,\n",
        "                       enable_progress_bar=False,\n",
        "                       callbacks=[ErrorTracker()])\n",
        "imdb_trainer.fit(imdb_module,\n",
        "                 datamodule=imdb_dm)\n",
        "test_results = imdb_trainer.test(imdb_module, datamodule=imdb_dm)\n",
        "test_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "dlQRl0HFrvI9",
        "outputId": "a966f1f1-94c1-4011-8fb0-d5bd24e5c8d9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type              | Params | Mode \n",
            "----------------------------------------------------\n",
            "0 | model | IMDBModel         | 644 K  | train\n",
            "1 | loss  | BCEWithLogitsLoss | 0      | train\n",
            "----------------------------------------------------\n",
            "644 K     Trainable params\n",
            "0         Non-trainable params\n",
            "644 K     Total params\n",
            "2.578     Total estimated model params size (MB)\n",
            "6         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=30` reached.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8586400151252747    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.54632568359375     \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8586400151252747     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.54632568359375      </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'test_loss': 1.54632568359375, 'test_accuracy': 0.8586400151252747}]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size):\n",
        "        super(IMDBModel, self).__init__()\n",
        "        self.dense1 = nn.Linear(input_size, 64) # Changed to 64 units\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3) # Dropout regularisation\n",
        "        self.dense2 = nn.Linear(64, 64) # Changed to 64 units\n",
        "        self.output = nn.Linear(64, 1) # Changed to 64 units\n",
        "\n",
        "    def forward(self, x):\n",
        "        val = x\n",
        "        for _map in [self.dense1,\n",
        "                     self.activation,\n",
        "                     self.dropout,\n",
        "                     self.dense2,\n",
        "                     self.activation,\n",
        "                     self.dropout,\n",
        "                     self.output]:\n",
        "            val = _map(val)\n",
        "        return torch.flatten(val)\n",
        "imdb_model = IMDBModel(imdb_test.tensors[0].size()[1])\n",
        "summary(imdb_model,\n",
        "        input_size=imdb_test.tensors[0].size(),\n",
        "        col_names=['input_size',\n",
        "                   'output_size',\n",
        "                   'num_params'])\n",
        "imdb_optimizer = RMSprop(imdb_model.parameters(), lr=0.001)\n",
        "imdb_module = SimpleModule.binary_classification(\n",
        "                         imdb_model,\n",
        "                         optimizer=imdb_optimizer)\n",
        "imdb_logger = CSVLogger('logs', name='IMDB')\n",
        "imdb_trainer = Trainer(deterministic=True,\n",
        "                       max_epochs=30,\n",
        "                       logger=imdb_logger,\n",
        "                       enable_progress_bar=False,\n",
        "                       callbacks=[ErrorTracker()])\n",
        "imdb_trainer.fit(imdb_module,\n",
        "                 datamodule=imdb_dm)\n",
        "test_results = imdb_trainer.test(imdb_module, datamodule=imdb_dm)\n",
        "test_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "GXeu6xP0rrZf",
        "outputId": "b89a803b-68f4-4a85-8591-a4bba4dfa4ea"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type              | Params | Mode \n",
            "----------------------------------------------------\n",
            "0 | model | IMDBModel         | 644 K  | train\n",
            "1 | loss  | BCEWithLogitsLoss | 0      | train\n",
            "----------------------------------------------------\n",
            "644 K     Trainable params\n",
            "0         Non-trainable params\n",
            "644 K     Total params\n",
            "2.578     Total estimated model params size (MB)\n",
            "7         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=30` reached.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8691999912261963    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9133929014205933    \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8691999912261963     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9133929014205933     </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'test_loss': 0.9133929014205933, 'test_accuracy': 0.8691999912261963}]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Comparison of Model Configurations:\n",
        "\n",
        "| Hidden Units | Dropout | Test Accuracy | Test Loss       |\n",
        "|--------------|---------|---------------|-----------------|\n",
        "| 16           | 0.0     | 0.8450        | 1.2168          |\n",
        "| 32           | 0.3     | 0.8643        | 0.9322          |\n",
        "| 64           | 0.0     | 0.8586        | 1.5463          |\n",
        "| 64           | 0.3     | 0.8692        | 0.9134          |\n",
        "\n",
        "For Optimal Performance:\n",
        "\n",
        "- Utilize 64 hidden units with 30% dropout to achieve the best trade-off between model complexity and generalization.\n",
        "\n",
        "For Faster Training with Adequate Performance:\n",
        "\n",
        "- 32 hidden units with 30% dropout also provide a strong performance with potentially lower computational overhead compared to the 64-unit model.\n",
        "\n",
        "Avoid Models Without Dropout When Using Higher Hidden Units:\n",
        "\n",
        "- As seen with the 64-unit model without dropout, higher hidden units can lead to overfitting, resulting in poorer generalization as indicated by increased loss.\n"
      ],
      "metadata": {
        "id": "Vb73ao62rzq5"
      }
    }
  ]
}